# Axolotl config for fine-tuning Magistral-Small-2509 on epistemic stance classification
# Hardware: 4x H100 80GB SXM5 (320 GB VRAM total) - Full fine-tune
#
# IMPORTANT: Magistral-Small-2509 is a MULTIMODAL model (version 1.2 added vision)
# Even for text-only training, we must configure it as multimodal per Axolotl docs.
#
# PREREQUISITE: Install vision dependencies:
#   pip install 'mistral-common[opencv]==1.8.5'

base_model: mistralai/Magistral-Small-2509

# DO NOT specify model_type or tokenizer_type for Magistral - let Axolotl auto-detect
# The official example only specifies base_model
tokenizer_use_mistral_common: true

# REQUIRED for multimodal models (per Axolotl docs)
processor_type: AutoProcessor
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false  # Not supported with multimodal

# Full fine-tune in bfloat16
load_in_8bit: false
load_in_4bit: false
bf16: auto
fp16: false
tf32: true

# Dataset configuration
datasets:
  - path: final_training_data_formatted.jsonl
    type: chat_template
    chat_template: chatml
    field_messages: conversations
    message_property_mappings:
      role: from
      content: value

# Dataset processing
dataset_prepared_path: ./prepared_data
val_set_size: 0.1
shuffle_merged_datasets: true

# Sequence length
sequence_len: 2048
pad_to_sequence_len: true

# Training hyperparameters
num_epochs: 3
micro_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2e-5
lr_scheduler: cosine
warmup_ratio: 0.1
weight_decay: 0.01
max_grad_norm: 1.0

# Optimizer
optimizer: adamw_torch_fused

# Memory optimization
gradient_checkpointing: true
flash_attention: false

# No LoRA - full fine-tune
adapter:

# DeepSpeed configuration for multi-GPU
deepspeed: deepspeed_configs/zero2.json

# Logging
logging_steps: 10
eval_steps: 100
save_steps: 100
save_total_limit: 3

# Output
output_dir: ./epistemic_stance_model

# Evaluation
eval_strategy: steps
eval_sample_packing: false

# Hub upload (optional)
hub_model_id: johnclund/epistemic-stance-analyzer
push_to_hub: true

# W&B logging (optional)
wandb_project: epistemic-stance
wandb_run_id:
wandb_watch:
wandb_log_model:
