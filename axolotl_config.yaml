# Axolotl config for fine-tuning Magistral-Small-2509 on epistemic stance classification
# Hardware: 4x H100 80GB SXM5 (320 GB VRAM total) - Full fine-tune
#
# NOTE: Magistral-Small-2509 is a MULTIMODAL model (vision + text) even though
# we're training with text-only data. We must configure it as multimodal.

base_model: mistralai/Magistral-Small-2509
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
tokenizer_use_mistral_common: true

# CRITICAL: Magistral-Small-2509 is multimodal - must use AutoProcessor
processor_type: AutoProcessor

# Multimodal-specific settings (required even for text-only training)
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false  # Not supported with multimodal models

# Full fine-tune in bfloat16 (4x H100 has enough VRAM)
load_in_8bit: false
load_in_4bit: false
bf16: auto
fp16: false
tf32: true

# Dataset configuration
datasets:
  - path: final_training_data_formatted.jsonl
    type: chat_template
    chat_template: chatml
    field_messages: conversations
    message_property_mappings:
      role: from
      content: value

# Dataset processing
dataset_prepared_path: ./prepared_data
val_set_size: 0.1
shuffle_merged_datasets: true

# Sequence length
sequence_len: 2048
pad_to_sequence_len: true

# Training hyperparameters
num_epochs: 3
micro_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2e-5
lr_scheduler: cosine
warmup_ratio: 0.1
weight_decay: 0.01
max_grad_norm: 1.0

# Optimizer
optimizer: adamw_torch_fused

# Memory optimization
gradient_checkpointing: true
flash_attention: false  # Use SDPA instead

# No LoRA - full fine-tune (4x H100 has enough VRAM)
adapter:

# DeepSpeed configuration for multi-GPU
deepspeed: deepspeed_configs/zero2.json

# Logging
logging_steps: 10
eval_steps: 100
save_steps: 100
save_total_limit: 3

# Output
output_dir: ./epistemic_stance_model

# Evaluation
eval_strategy: steps
eval_sample_packing: false

# Hub upload (optional)
hub_model_id: johnclund/epistemic-stance-analyzer
push_to_hub: true

# W&B logging (optional)
wandb_project: epistemic-stance
wandb_run_id:
wandb_watch:
wandb_log_model:

# Note: special_tokens override is not supported with mistral-common tokenizer
# The tokenizer handles padding automatically