# Axolotl config for fine-tuning Mistral-Small-24B-Instruct-2501 on epistemic stance classification
# Hardware: 4x H100 80GB SXM5 (320 GB VRAM total) - Full fine-tune
#
# Mistral-Small-24B-Instruct-2501 (Mistral Small 3):
# - 24B parameters, text-only model
# - Uses Tekken V7 tokenizer (131k vocab)
# - 32k context window
# - ~55GB VRAM in bf16
# - Apache 2.0 license (no gated access)

base_model: mistralai/Mistral-Small-24B-Instruct-2501
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Liger kernel for memory optimization (recommended for Mistral training)
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true  # Updated from deprecated liger_swiglu
liger_fused_linear_cross_entropy: true

# Full fine-tune in bfloat16 (4x H100 has enough VRAM)
load_in_8bit: false
load_in_4bit: false
bf16: auto
fp16: false
tf32: true

# Dataset configuration
# Using tokenizer_default to use the model's built-in Tekken V7 chat template
datasets:
  - path: final_training_data_formatted.jsonl
    type: chat_template
    chat_template: tokenizer_default
    field_messages: conversations
    # Updated from deprecated message_field_role/message_field_content
    message_property_mappings:
      role: from
      content: value
    roles_to_train: ["gpt"]
    train_on_eos: turn

# Dataset processing
dataset_prepared_path: ./prepared_data
val_set_size: 0.1
shuffle_merged_datasets: true

# Sequence length - Mistral Small 3 supports 32k, but 2048 is enough for our classification task
sequence_len: 2048
sample_packing: true
eval_sample_packing: false
pad_to_sequence_len: true

# Training hyperparameters
num_epochs: 3
micro_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2e-5
lr_scheduler: cosine
warmup_ratio: 0.1
weight_decay: 0.0
max_grad_norm: 1.0

# Optimizer
optimizer: adamw_torch_fused

# Memory optimization
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
flash_attention: false
sdp_attention: true

# Training settings
train_on_inputs: false
group_by_length: false

# No LoRA - full fine-tune (4x H100 has enough VRAM)
adapter:

# DeepSpeed configuration for multi-GPU
deepspeed: deepspeed_configs/zero2.json

# Logging
logging_steps: 10
eval_steps: 100

# Saving - use saves_per_epoch (save_steps is mutually exclusive)
saves_per_epoch: 2
save_total_limit: 3

# Output
output_dir: ./epistemic_stance_model

# Evaluation
eval_strategy: steps

# Hub upload (optional)
hub_model_id: johnclund/epistemic-stance-analyzer
push_to_hub: true

# W&B logging (optional)
wandb_project: epistemic-stance
wandb_run_id:
wandb_watch:
wandb_log_model:

# Special tokens - let the tokenizer handle padding
special_tokens:
  pad_token: "<pad>"